{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T501-SP24 - Lab Assignment 4.2 \n",
    "## *Collaborative Filtering with Spark MLlib and ALS*\n",
    "\n",
    "*This code is based on an example at Based on code from https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html; a modified version is part of the tutorial in CloudxLab.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(230, 230, 230);\">\n",
    "\n",
    "---\n",
    "## Assignment Details\n",
    "\n",
    "### Objectives\n",
    "The learning objectives of this assignment are:\n",
    "1.\tto gain experience running a Spark MLlib notebook in Jupyter & CloudxLab\n",
    "2.  to experiment with the impact of various ML model parameters with the goal of finding an practical balance of speed and accuracy\n",
    "3.  to work successfully in a team on a collaborative assignment\n",
    "\n",
    "### General Instructions\n",
    "* Read and run the code in **Sections 1-3**. This code is based on an example at Based on code from https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html; a modified version is part of the tutorial in CloudxLab.\n",
    "* In **Section 4**, you are asked to repeatedly adjust model parameters and train and test the model to determine the effects of various parameter settings. You are then asked to answer some basic questions about the results.\n",
    "* As with our other open-ended questions and assignments, do not try to find a single right answer. Instead, pay attention to the results of your work and make intelligent adjustments and analyses based on the results you get.\n",
    "* Upload this Jupyter notebook to your Jupyter directory on CloudXLab. Enter all of your code into this notebook. You should not need to download any datasets. The given notebook uses an existing dataset installed in CloudxLab\n",
    "* Your modified Jupyter notebook will serve as your groupâ€™s submission for the assignment. You should **also submit a PDF version** of the notebook (through \"print\" to a PDF or your system, or download as PDF from CloudXLab.)\n",
    "\n",
    "---\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Prep Environment\n",
    "### 1.1 Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.recommendation.ALS \n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import scala.math._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define new data class and function to work with Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Rating\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "parseRating: (str: String)Rating\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Rating(1,1193,5.0,978300760)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define a new class called \"Rating\"\n",
    "case class Rating(userId: Int, movieId: Int, rating: Float, timestamp: Long)\n",
    "\n",
    "// define a function that takes a single string and splits it on the \"::\" separator \n",
    "//   into the 4 elements of a Rating (userID, movieID, rating, timestamp)\n",
    "def parseRating(str: String): Rating = {\n",
    "  val fields = str.split(\"::\")\n",
    "  assert(fields.size == 4)\n",
    "  Rating(fields(0).toInt, fields(1).toInt, fields(2).toFloat, fields(3).toLong)\n",
    "}\n",
    "\n",
    "//Test the new function\n",
    "parseRating(\"1::1193::5::978300760\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Read and Modify Data\n",
    "### 2.1 Read text file with ratings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1::1193::5::978300760\n",
      "1::661::3::978302109\n",
      "1::914::3::978301968\n",
      "1::3408::4::978300275\n",
      "1::2355::5::978824291\n",
      "========\n",
      "Number of ratings: 1000209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rawRatingsRDD = /data/ml-1m/ratings.dat MapPartitionsRDD[1] at textFile at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/data/ml-1m/ratings.dat MapPartitionsRDD[1] at textFile at <console>:32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var rawRatingsRDD = sc.textFile(\"/data/ml-1m/ratings.dat\")\n",
    "\n",
    "// Check one record\n",
    "//  - it should be res4: Array[String] = Array(1::1193::5::978300760)\n",
    "// If this fails the location of file is wrong.\n",
    "rawRatingsRDD.take(5).foreach(println)\n",
    "println(\"========\")\n",
    "println(\"Number of ratings: \" + rawRatingsRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Parse the RDD, convert to DataFrame, and display a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|   1193|   5.0|978300760|\n",
      "|     1|    594|   4.0|978302268|\n",
      "|     1|   3186|   4.0|978300019|\n",
      "|     1|    588|   4.0|978824268|\n",
      "|     1|   1836|   5.0|978300172|\n",
      "|     1|   1022|   5.0|978300055|\n",
      "|     1|   1246|   4.0|978302091|\n",
      "|     2|    647|   3.0|978299351|\n",
      "|     2|   2628|   3.0|978300051|\n",
      "|     2|   3107|   2.0|978300002|\n",
      "+------+-------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ratingsDF = [userId: int, movieId: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: int, movieId: int ... 2 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ratingsDF = rawRatingsRDD.map(parseRating).toDF()\n",
    "//check if everything is ok\n",
    "ratingsDF.sample(false,0.1).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Divide the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 2) / 2]\n",
      "Rows of training data: 799915\n",
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|    595|   5.0|978824268|\n",
      "|     2|    356|   5.0|978299686|\n",
      "|     2|   1198|   4.0|978298124|\n",
      "|     2|   1245|   2.0|978299200|\n",
      "|     2|   1259|   5.0|978298841|\n",
      "|     2|   1945|   5.0|978298458|\n",
      "|     2|   1962|   5.0|978298813|\n",
      "|     2|   2006|   3.0|978299861|\n",
      "|     2|   3699|   2.0|978299173|\n",
      "|     2|   3893|   1.0|978299535|\n",
      "+------+-------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Rows of test data: 200294                                                       \n",
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|     48|   5.0|978824351|\n",
      "|     2|    589|   4.0|978299773|\n",
      "|     2|   1917|   3.0|978300174|\n",
      "|     2|   2571|   4.0|978299773|\n",
      "|     5|     36|   3.0|978244808|\n",
      "|     5|    357|   2.0|978245829|\n",
      "|     5|   3260|   4.0|978245065|\n",
      "|     5|   3728|   2.0|978244568|\n",
      "|     6|   2858|   1.0|978236809|\n",
      "|     7|   1221|   4.0|978234659|\n",
      "+------+-------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainingData = [userId: int, movieId: int ... 2 more fields]\n",
       "testData = [userId: int, movieId: int ... 2 more fields]\n",
       "trainingDataCount = 799915\n",
       "testDataCount = 200294\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "200294"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// split the data into 80% for training and 20% for testing\n",
    "val Array(trainingData, testData) = ratingsDF.randomSplit(Array(0.8, 0.2))\n",
    "// show 10 randomly sampled rows from each\n",
    "val trainingDataCount = trainingData.count()\n",
    "println(\"\\nRows of training data: \" + trainingDataCount)\n",
    "trainingData.sample(false,0.1).show(10)\n",
    "\n",
    "val testDataCount = testData.count()\n",
    "println(\"Rows of test data: \" + testDataCount)\n",
    "testData.sample(false,0.1).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Alternating Least Squares (ALS) Machine Learning Model\n",
    "### 3.1 Create ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratingsALS = als_73effdc5b8e0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "als_73effdc5b8e0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create ALS with default parameter values\n",
    "\n",
    "val ratingsALS = new ALS()\n",
    "    .setMaxIter(10)\n",
    "    .setRank(10)\n",
    "    .setRegParam(0.1)\n",
    "    .setUserCol(\"userId\")\n",
    "    .setItemCol(\"movieId\")\n",
    "    .setRatingCol(\"rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train ALS with Training portion of data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                        (0 + 0) / 10]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "myALSmodel = als_73effdc5b8e0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "als_73effdc5b8e0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myALSmodel = ratingsALS.fit(trainingData)\n",
    "//you can also save the model for future replication\n",
    "//myALSmodel.save(\"myALSmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run model on the Test portion of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 90:=====================================================>(197 + 3) / 200]+------+-------+------+---------+----------+\n",
      "|userId|movieId|rating|timestamp|prediction|\n",
      "+------+-------+------+---------+----------+\n",
      "|    26|    463|   3.0|978271588| 2.9657395|\n",
      "|  3841|    471|   4.0|965996668| 3.3257422|\n",
      "|  4227|    471|   3.0|965319625| 2.7798407|\n",
      "|   157|    471|   3.0|977249978| 3.6272252|\n",
      "|  1939|    471|   4.0|974695248|  3.629592|\n",
      "|  2941|    471|   5.0|971348751| 3.5521705|\n",
      "|  2967|    471|   1.0|971133756| 3.2005475|\n",
      "|  4647|    471|   2.0|963978127|  3.490961|\n",
      "|  4480|    471|   3.0|965026114|  3.411218|\n",
      "|  1377|    471|   1.0|974770883|  2.449363|\n",
      "+------+-------+------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictedRatings = [userId: int, movieId: int ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: int, movieId: int ... 3 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Prepare the predicted ratings \n",
    "val predictedRatings = myALSmodel.transform(testData)\n",
    "\n",
    "// show a sampling of results; note the new prediction column\n",
    "predictedRatings.sample(false,0.1).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Compute the difference between the actual and predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (root mean squared error) = 0.8709834075972182                             \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sumMSE = 151945.45\n",
       "numVals = 200294\n",
       "MSE = 0.7586121\n",
       "RMSE = 0.8709834075972182\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8709834075972182"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Roll our own RMSE calculation as the RegressionEvaluator often returns NaN\n",
    "\n",
    "// compute difference between actual (column 2) and predicted (column 4)\n",
    "var sumMSE = predictedRatings\n",
    "    .map(r => r(2).asInstanceOf[Float] - r(4).asInstanceOf[Float])\n",
    "    .map(x => x*x)\n",
    "    .filter(!_.isNaN)\n",
    "    .reduce(_ + _)\n",
    "\n",
    "var numVals = predictedRatings.count()\n",
    "var MSE = sumMSE/numVals\n",
    "var RMSE = sqrt(MSE)\n",
    "println(\"RMSE (root mean squared error) = \" + RMSE)\n",
    "\n",
    "// below  is the prescribed way to compute RMSE, but it often returns NaN because of empty values \n",
    "/*\n",
    "val evaluator = new RegressionEvaluator()\n",
    "  .setMetricName(\"rmse\")\n",
    "  .setLabelCol(\"rating\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "val RMSE2 = evaluator.evaluate(predictedRatings)\n",
    "println(s\"Root-mean-square error = $RMSE2\")\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Show difference between actual and predicted for a sampling of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 179:===================================================> (194 + 6) / 200]+------+-------+------+----------+-----------+\n",
      "|userId|movieId|rating|prediction|      error|\n",
      "+------+-------+------+----------+-----------+\n",
      "|  5367|    471|   3.0| 3.1691663|-0.16916633|\n",
      "|  1475|    471|   5.0|  3.668871|  1.3311291|\n",
      "|  2174|    471|   3.0| 3.4339888| -0.4339888|\n",
      "|  3777|    471|   3.0| 3.2297559|-0.22975588|\n",
      "|  1790|    471|   4.0|  3.516504| 0.48349595|\n",
      "|  5718|    471|   4.0| 3.4710252|  0.5289748|\n",
      "|  1980|    471|   4.0| 3.5605521| 0.43944788|\n",
      "|  4728|    471|   2.0| 3.5355308| -1.5355308|\n",
      "|  2411|    471|   3.0| 3.5146306|-0.51463056|\n",
      "|   294|    471|   3.0| 3.6393967|-0.63939667|\n",
      "|  5530|    471|   5.0| 3.5013292|  1.4986708|\n",
      "|   928|    496|   4.0|   3.55404| 0.44596004|\n",
      "|  2024|    833|   1.0| 2.2018604| -1.2018604|\n",
      "|  3512|   1088|   3.0|  3.230005|-0.23000503|\n",
      "|  2724|   1088|   3.0| 3.0706909|-0.07069087|\n",
      "|  4882|   1088|   4.0| 2.9115334|  1.0884666|\n",
      "|  4786|   1088|   3.0| 3.4856484| -0.4856484|\n",
      "|   531|   1088|   4.0|  3.359896| 0.64010406|\n",
      "|  5840|   1088|   5.0| 3.5951297|  1.4048703|\n",
      "|  2793|   1088|   4.0| 3.4283023|  0.5716977|\n",
      "|  2125|   1088|   3.0| 3.4271634|-0.42716336|\n",
      "|  4728|   1088|   3.0|  2.976852| 0.02314806|\n",
      "|  5026|   1088|   3.0| 2.7944267| 0.20557332|\n",
      "|  4064|   1088|   2.0|  2.436082| -0.4360819|\n",
      "|   991|   1088|   3.0| 3.4590843|-0.45908427|\n",
      "|  1591|   1238|   5.0|  4.585079|  0.4149208|\n",
      "|  2185|   1238|   5.0|  4.232067|  0.7679329|\n",
      "|  2849|   1238|   5.0|  4.404795| 0.59520483|\n",
      "|  1632|   1238|   3.0| 3.1552286|-0.15522861|\n",
      "|  5494|   1238|   3.0| 3.8509228| -0.8509228|\n",
      "+------+-------+------+----------+-----------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictedRatings\n",
    "    .select(\"userId\", \"movieId\", \"rating\", \"prediction\")\n",
    "    .withColumn(\"error\",$\"rating\"-$\"prediction\")\n",
    "    .sample(false,0.1)\n",
    "    .show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Use model to make recommendations for Users or for Items\n",
    "\n",
    "(This requires Spark 2.2 or later, so does not work in CloudxLab (Spark 1.63) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "// Generate top 10 movie recommendations for each user\n",
    "val userRecs = myALSmodel.recommendForAllUsers(10)\n",
    "// Generate top 10 user recommendations for each movie\n",
    "val movieRecs = myALSmodel.recommendForAllItems(10)\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## 4. Your Assignment: Run a \"Parameter Sweep\" of ALS Models\n",
    "Your assignment is to run the following code block multiple times with different parameters to determine a reasonable tradeoff between accuracy and time. \n",
    "Note the following:\n",
    "* Obviously, it is not necessary to read the data each time.\n",
    "* We **do not** want to repeat the splitting process each time as that will result in different training and test data sets.\n",
    "* We can use the \"currentTimeMillis()\" function to get an *approximate* timing for the computation. (The shared nature and varying load of CloudxLab resources is a confounding factor to timing.)\n",
    "* We include windows covering the creation, training, prediction, and error measurement stages.\n",
    "\n",
    "Prediction performance of a Spark ALS model is affected by the following parameters\n",
    "\n",
    "|Parameter|Description|Default value|Notes |\n",
    "|:-----|:-----|:-----:|:----- |\n",
    "|rank|Number of latent factors|10|The larger the value, the more intrinsic factors considered in the factorization modeling.|\n",
    "|regParam|Regularization parameter|1.0|The value needs to be selected empirically to avoid overfitting.|\n",
    "|maxIters|Maximum number of iterations | 10 |The more iterations the better the model converges to the optimal point.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Collect timings\n",
    "Run the following code block **at least 12 times** with a variety of parameters. Copy and paste the output from each run into the markdown table below. \n",
    "\n",
    "#### Collaborative Filtering ALS Parameter Sweep \n",
    "\n",
    "| Run | MaxIter | Rank | RegParam | RMSE | time_create | time_fit | time_predict | time_RMSE |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| x | x | x | x | x | x | x | x | x |\n",
    "| x | x | x | x | x | x | x | x | x |\n",
    "| x | x | x | x | x | x | x | x | x |\n",
    "| 1.1 | 1 | 2 | 1.0|   3.7382 | 0.007 | 5.678 | 0.048 | 22.014 |\n",
    "| 1.2 | 5 | 5 | 0.1|   0.8863 | 0.004 | 8.996 | 0.049 | 18.612 |\n",
    "| 1.3 | 5 | 10 | 0.1|   0.8916 | 0.003 | 5.038 | 0.037 | 18.765 |\n",
    "| 1.4 | 5 | 20 | 0.1|   0.8920 | 0.006 | 8.229 | 0.043 | 20.878 |\n",
    "| 1.5 | 10 | 5 | 0.5|   1.0410 | 0.002 | 7.477 | 0.033 | 18.211 |\n",
    "| 1.6 | 10 | 10 | 0.5|   1.0408 | 0.002 | 9.624 | 0.036 | 13.98 |\n",
    "| 1.7 | 10 | 20 | 0.5|   1.0384 | 0.004 | 11.586 | 0.045 | 18.28 |\n",
    "| 1.8 | 20 | 5 | 1|   1.3526 | 0.004 | 11.434 | 0.043 | 17.846 |\n",
    "| 1.9 | 20 | 10 | 1|   1.3526 | 0.003 | 12.155 | 0.04 | 16.002 |\n",
    "| 1.10 | 20 | 20 | 1|   1.3526 | 0.002 | 15.016 | 0.033 | 19.728 |\n",
    "| 1.11 | 3 | 10 | 0.05|   0.8849 | 0.004 | 8.121 | 0.046 | 21.112 |\n",
    "| 1.12 | 15 | 15 | 0.2|   0.9139 | 0.004 | 11.999 | 0.047 | 20.573 |\n",
    "| 1.13 | 10 | 15 | 0.8|   1.2130 | 0.003 | 9.124 | 0.036 | 17.912 |\n",
    "| 1.14 | 15 | 5 | 0.05|   0.8643 | 0.003 | 10.035 | 0.037 | 19.373 |\n",
    "| 1.15 | 15 | 10 | 0.05|   0.8558 | 0.003 | 10.778 | 0.044 | 18.145 |\n",
    "| 1.16 | 15 | 20 | 0.05|   0.8577 | 0.004 | 12.219 | 0.027 | 15.792 |\n",
    "| 1.17 | 10 | 5 | 0.1|   0.8771 | 0.003 | 6.863 | 0.053 | 13.873 |\n",
    "| 1.18 | 20 | 5 | 0.1|   0.8722 | 0.004 | 12.634 | 0.035 | 18.854 |\n",
    "| 1.19 | 1 | 5 | 0.1|   3.3544 | 0.003 | 7.05 | 0.038 | 18.162 |\n",
    "#### Optimal / Recommended settings for given dataset\n",
    "_From your table above, copy the parameter combination that minimizes the error in the most reasonable amount of time_\n",
    "\n",
    "| Run | MaxIter | Rank | RegParam | RMSE | time_create | time_fit | time_predict | time_RMSE |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| x | x | x | x | x | x | x | x | x |\n",
    "| 1.15 | 15 | 10 | 0.05|   0.8558 | 0.003 | 10.778 | 0.044 | 18.145 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (root mean squared error) = 0.8714798554867962                             \n",
      "| Run | MaxIter | Rank | RegParam | RMSE | time_create | time_fit | time_predict | time_RMSE |\n",
      "| 1.20 | 25 | 5 | 0.1|   0.8715 | 0.003 | 12.528 | 0.029 | 22.058 |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "runName = 1.20\n",
       "maxIter = 25\n",
       "rank = 5\n",
       "regParam = 0.1\n",
       "t0 = 1713415068387\n",
       "ratingsALS = als_305314dc6f0a\n",
       "t1 = 1713415068390\n",
       "myALSmodel = als_305314dc6f0a\n",
       "t2 = 1713415080918\n",
       "predictedRatings = [userId: int, movieId: int ... 3 more fields]\n",
       "t3 = 1713415080947\n",
       "sumMSE = 152118.72\n",
       "numVals = 200294\n",
       "MSE = 0.75947714\n",
       "RMSE = 0.8714798554867962\n",
       "t4 = 1713415103005\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1713415103005"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// change the following four values\n",
    "var runName = \"1.20\"  //any string that makes sense to you\n",
    "// the parameters for the ALS system\n",
    "// see https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html for details\n",
    "var maxIter = 25 // maximum number of iterations of the alternating algorithm\n",
    "var rank = 5 // also known as number of latent factors; default is 10\n",
    "var regParam = .1 // regularization parameter; default is 0.1\n",
    "\n",
    "var t0 = System.currentTimeMillis()\n",
    "\n",
    "val ratingsALS = new ALS()\n",
    "    .setMaxIter(maxIter)\n",
    "    .setRank(rank)\n",
    "    .setRegParam(regParam)\n",
    "    .setUserCol(\"userId\")\n",
    "    .setItemCol(\"movieId\")\n",
    "    .setRatingCol(\"rating\")\n",
    "\n",
    "val t1 = System.currentTimeMillis()\n",
    "\n",
    "val myALSmodel = ratingsALS.fit(trainingData)\n",
    "\n",
    "val t2 = System.currentTimeMillis()\n",
    "\n",
    "val predictedRatings = myALSmodel.transform(testData)\n",
    "\n",
    "val t3 = System.currentTimeMillis()\n",
    "\n",
    "var sumMSE = predictedRatings\n",
    "    .map(r => r(2).asInstanceOf[Float] - r(4).asInstanceOf[Float])\n",
    "    .map(x => x*x)\n",
    "    .filter(!_.isNaN)\n",
    "    .reduce(_ + _)\n",
    "var numVals = testDataCount // should really divide by predictedRatings.count(), but close enough\n",
    "var MSE = sumMSE/numVals\n",
    "var RMSE = sqrt(MSE)\n",
    "println(\"RMSE (root mean squared error) = \" + RMSE)\n",
    "\n",
    "val t4 = System.currentTimeMillis()\n",
    "\n",
    "println(\"| Run | MaxIter | Rank | RegParam | RMSE | time_create | time_fit | time_predict | time_RMSE |\")\n",
    "print(\"| \" + runName + \" | \" + maxIter + \" | \" + rank + \" | \" + regParam)\n",
    "print(f\"| $RMSE%8.4f\")\n",
    "print(\" | \" + ((t1-t0)/1000.0) )\n",
    "print(\" | \" + ((t2-t1)/1000.0) )\n",
    "print(\" | \" + ((t3-t2)/1000.0) )\n",
    "print(\" | \" + ((t4-t3)/1000.0) )\n",
    "println(\" |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.2 Analyze Results\n",
    "\n",
    "*Answer each of the following in several complete sentences. Please refer to specific values or results in your table when justifying your answers.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 General Trends\n",
    "What general trends did you notice? Do these trends seem reasonable to you? Why or why not?\n",
    "\n",
    "(Type your answer here)\n",
    "\n",
    "Examining the data reveals some general trends related to the interaction between the ALS model parameters (`MaxIter`, `Rank`, `RegParam`) and the resulting RMSE. Notably, increasing `MaxIter` tends to greatly reduce RMSE from 1-5 iterations, and slightly reduces RMSE from 5-10 iterations. This suggests more iterations allow the model to converge better on the training data, seen in runs from 1.1 to 1.10 and 1.17 to 1.19 where iterations increase from 1 to 20. However, this improvement plateaus, indicating diminishing returns after a certain point.\n",
    "\n",
    "The `Rank` parameter, which determines the complexity of the model, shows that a middle range (about 10-15) tends to offer the best balance between underfitting and overfitting. For instance, runs 1.11 and 1.14-1.16, with ranks around 10-15, have some of the lowest RMSEs.\n",
    "\n",
    "Regarding `RegParam`, lower values (around 0.05-0.1) correspond to lower RMSEs, suggesting that too much regularization (higher values like 0.5 or 1.0 in runs 1.5-1.10) can prevent the model from fitting well to the data.\n",
    "\n",
    "These trends appear reasonable as they align with typical expectations of how these parameters influence model performance in collaborative filtering tasks. Adjusting these parameters allows for fine-tuning the model's ability to generalize without overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Most time-consuming task\n",
    "Which part or parts of the workflow took the *most* time? What do you think accounts for that?\n",
    "\n",
    "(Type your answer here)\n",
    "\n",
    "The most time-consuming task in the workflow is the calculation of RMSE, as evidenced by the 'time_RMSE' column, where values are notably high, such as 22.014 seconds in run 1.1. This phase is particularly intensive because it involves complex operations for each predictionâ€”calculating errors, squaring them, and handling exceptions like NaN values, which can be computationally demanding. The lengthy duration for RMSE calculations arises from the need to process each element in a potentially large dataset comprehensively. This task's duration is affected by the dataset's size and the complexity of the operations required to compute the mean squared error across all predictions. This highlights the impact of data handling and error computation tasks on the overall performance of the model evaluation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Least time-consuming task\n",
    "Which part or parts of the workflow took the *least* time? What do you think accounts for that?\n",
    "\n",
    "(Type your answer here)\n",
    "\n",
    "The least time-consuming part of the workflow is consistently the model creation step, as indicated by the 'time_create' column. For instance, in run 1.16, this step took only 0.004 seconds. This step involves setting up the ALS model with specified parameters such as `MaxIter`, `Rank`, and `RegParam`, which is relatively quick because it primarily consists of initializing settings rather than performing any intensive computations. The simplicity and minimal computational demand of initializing parameters, as opposed to executing complex calculations or iterative processes found in fitting and predicting stages, account for the short duration of this step. This quick initialization is typical in machine learning workflows where the bulk of the computational effort is spent on training and predicting rather than on the setup phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4 Most variable task\n",
    "Which part or parts of the workflow had the greatest variation in time? What do you think accounts for that?\n",
    "\n",
    "(Type your answer here)\n",
    "\n",
    "The 'time_fit' part of the workflow showed the greatest variation in time across different runs, as seen in the significant differences between runs such as 1.10 (15.016 seconds) compared to others like 1.1 (5.678 seconds). This variability can be attributed to the different configurations of the `MaxIter`, `Rank`, and `RegParam` parameters. The `MaxIter` influences how many times the algorithm iteratively adjusts the model to minimize errors, with more iterations potentially leading to longer fitting times. The `Rank` affects the complexity of the model, with higher ranks increasing the number of latent factors to compute, thereby extending the fitting duration. The varying combinations of these parameters across different runs result in different computational loads and optimization challenges, explaining the observed variability in the model fitting times. This stage's time consumption is inherently sensitive to the algorithm's configuration and the data's characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.5 Recommended set of parameters\n",
    "For the given dataset, what do you think is the optimal parameter combination that minimizes the error in the most reasonable amount of time?\n",
    "\n",
    "(Type your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the observed results and underlying logic of parameter influence, an optimal combination of parameters that balances error minimization and computational efficiency appears to be around a `MaxIter` of 15, a `Rank` of 10, and a `RegParam` of 0.05, as evidenced by run 1.15 which delivered an RMSE of 0.8558. This configuration presents a robust model with good generalization capabilities and efficient computation for the given dataset.\n",
    "\n",
    "However, for potential further optimization, I would consider the following slight adjustments:\n",
    "\n",
    "1. Slightly adjust `Rank`: Testing values slightly higher than 10, such as 12 or 15, might capture more complex patterns in the data without significant overfitting, potentially improving accuracy if the increase in computational time is justified.\n",
    "   \n",
    "2. Fine-tune `RegParam`: Experimenting within a tighter range around 0.05, such as 0.03 or 0.07, could refine the balance between model complexity and regularization strength, potentially squeezing additional performance out of the model.\n",
    "\n",
    "3. Experiment with `MaxIter`: Modifying iterations to either 18 or down to 12 could optimize the trade-off between convergence time and accuracy, based on the algorithmâ€™s convergence behavior observed in more detailed diagnostics.\n",
    "\n",
    "These adjustments encourage iterative refinement in tuning the machine learning model, aiming for even better predictive accuracy while maintaining reasonable computational demands.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
